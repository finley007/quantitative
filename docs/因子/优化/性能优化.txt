性能优化
115
609.48032390 s 打日志
620.13436480 s 不打日志
289.15890180 s 一次优化
241.25521040 s 二次优化

107
523.11037700 s 打日志
541.69034410 s 不打日志

更改代码之前先保存一份便于比较逻辑

思路：
get_stock_tick_data 5.68763660 s 缓存
    并发加载文件测试：
    5 10
    test_loadfile_pte.py::test_sequence_run cost time: 46.62818500 s
    PASSED
    test_loadfile_pte.py::test_concurrency_thread_run Init thread runner for 10
    cost time: 36.31712110 s
    PASSED
    test_loadfile_pte.py::test_concurrency_process_run Init process runner for 10
    cost time: 31.76191690 s

    5 20
    test_loadfile_pte.py::test_sequence_run cost time: 47.23244180 s
    PASSED
    test_loadfile_pte.py::test_concurrency_thread_run Init thread runner for 20
    cost time: 38.21143670 s
    PASSED
    test_loadfile_pte.py::test_concurrency_process_run Init process runner for 20
    cost time: 32.05368170 s

    5 5
    test_loadfile_pte.py::test_sequence_run cost time: 46.23306500 s
    PASSED
    test_loadfile_pte.py::test_concurrency_thread_run Init thread runner for 5
    cost time: 35.35759530 s
    PASSED
    test_loadfile_pte.py::test_concurrency_process_run Init process runner for 5
    cost time: 30.74610300 s
    PASSED

    6 5
    test_loadfile_pte.py::test_sequence_run cost time: 58.21607720 s
    PASSED
    test_loadfile_pte.py::test_concurrency_thread_run Init thread runner for 5
    cost time: 43.62907570 s
    PASSED
    test_loadfile_pte.py::test_concurrency_process_run Init process runner for 5
    cost time: 36.40942240 s
    PASSED

    7 5
    test_loadfile_pte.py::test_sequence_run cost time: 68.81986520 s
    PASSED
    test_loadfile_pte.py::test_concurrency_thread_run Init thread runner for 5
    cost time: 54.33602080 s
    PASSED
    test_loadfile_pte.py::test_concurrency_process_run Init process runner for 5
    cost time: 43.91429590 s

    7 10
    test_loadfile_pte.py::test_sequence_run cost time: 69.53964820 s
    PASSED
    test_loadfile_pte.py::test_concurrency_thread_run Init thread runner for 10
    cost time: 53.11254400 s
    PASSED
    test_loadfile_pte.py::test_concurrency_process_run Init process runner for 10
    cost time: 43.04185860 s
    PASSED

    两种思路都尝试失败了
        1. 在caculate_by_date方法中用多线程（进程）提前加载文件，失败的原因是多线程对于一天的数据提升优先，多进程开销太大，因此都是不快反慢
        2. 在每一个分页里提前用多进程获取数据：40-42行
            def caculate(self, data):
                """
                现货因子计算逻辑，多进程按天计算
                Parameters
                ----------
                data

                Returns
                -------

                """
                columns = self.get_factor_columns(data)
                new_data = pd.DataFrame(columns=columns)
                product = data.iloc[0]['product']
                instrument = data.iloc[0]['instrument']
                date_list = list(set(data['date'].tolist()))
                date_list.sort()
                pagination = Pagination(date_list, page_size=20)
                while pagination.has_next():
                    date_list = pagination.next()
                    get_logger().debug('Handle date from {0} to {1} for instrument: {2}'.format(date_list[0], date_list[-1], instrument))
                    file_list = []
                    for date in date_list:
                        stock_list = self.get_stock_list_by_date(product, date)
                        file_list = file_list + list(map(lambda stock : (date, stock), stock_list))
                    batch_access = BatchStockDataAccess(file_list, concurrent_count = 10)
                    batch_stock_data = batch_access.batch_load_data()
                    params_list = list(map(lambda date: [date, instrument, product, batch_stock_data[date]], date_list))
                    results = ProcessExcecutor(10).execute(self.caculate_by_date, params_list)
                    temp_cache = {}
                    for result in results:
                        cur_date_data = self.merge_with_stock_data(data, result[0], result[1])
                        temp_cache[result[0]] = cur_date_data
                    for date in date_list:
                        new_data = pd.concat([new_data, temp_cache[date]])
                return new_data
        每一天省0.01 * 836 = 8s 多花费40s的数据加载时间

enrich_stock_data 改成numpy返回
根据line_profile的分析结果：
10458042.6  26276.5     44.2                  data = pd.concat([data, daily_stock_data])
可以把这个执行逻辑从循环里拿出来
7347922.5 7347922.5     35.2          stock_data_per_date_group_by = stock_data_per_date.groupby('cur_time')[change_columns].apply(
这个直接用numpy api
6012768.7  15107.5     37.1                      daily_stock_data = self.get_stock_data(date, stock)
这部分好像没有什么优化的余地了
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   123                                           @profile
   124                                           def method_name():
   125                                               global decompress
   126         1      10750.2  10750.2      7.2      decompress = read_decompress(
   127         1          0.1      0.1      0.0          'E:\\data\\organized\\stock\\tick\\stk_tick10_w_2020\\stk_tick10_w_202001\\20200102\\000031.pkl')
   128         1     116434.0 116434.0     77.8      decompress.to_csv('E:\\data\\temp\\test.csv')
   129         1      22564.1  22564.1     15.1      decompress1 = pd.read_csv('E:\\data\\temp\\test.csv')
pickel已经是最快的包，这个可以考虑用并发优化



